{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f4881a7",
   "metadata": {},
   "source": [
    "## üìò **Topic 3: Convergence Algorithm ‚Äì Gradient Descent**\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **What is Gradient Descent?**\n",
    "\n",
    "Gradient Descent is the **algorithm used to minimize the cost function** (like MSE) by **adjusting the parameters** ‚Äî in our case, the slope $m$ and intercept $b$ ‚Äî until the error is as small as possible.\n",
    "\n",
    "> It‚Äôs like **rolling downhill** on a cost curve until you reach the bottom ‚Äî the minimum error.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Goal**:\n",
    "\n",
    "Minimize the cost function $J(m, b)$ by updating:\n",
    "\n",
    "* Slope: $m$\n",
    "* Intercept: $b$\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Update Formulas**:\n",
    "\n",
    "$$\n",
    "m := m - \\alpha \\cdot \\frac{\\partial J}{\\partial m}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\alpha$ is the **learning rate** (step size)\n",
    "* $\\frac{\\partial J}{\\partial m}$ and $\\frac{\\partial J}{\\partial b}$ are the **gradients** (slopes of the cost function)\n",
    "\n",
    "---\n",
    "\n",
    "### üîß **Gradients for Linear Regression**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial m} = \\frac{2}{n} \\sum (mX_i + b - Y_i) \\cdot X_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{2}{n} \\sum (mX_i + b - Y_i)\n",
    "$$\n",
    "\n",
    "These tell us **how the cost changes** with respect to $m$ and $b$.\n",
    "\n",
    "---\n",
    "\n",
    "### üåÄ **Gradient Descent Algorithm (Steps)**:\n",
    "\n",
    "1. Initialize $m$ and $b$ to 0 (or random).\n",
    "2. Compute predicted $\\hat{Y}$ using current $m$ and $b$.\n",
    "3. Calculate the gradients.\n",
    "4. Update $m$ and $b$ using the gradients and learning rate.\n",
    "5. Repeat until cost converges (i.e., changes very little).\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example: Python Code for Gradient Descent\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "Y = np.array([40000, 50000, 60000, 70000, 80000])\n",
    "\n",
    "# Initialize parameters\n",
    "m = 0\n",
    "b = 0\n",
    "\n",
    "# Learning rate and iterations\n",
    "alpha = 0.01\n",
    "epochs = 1000\n",
    "n = len(X)\n",
    "\n",
    "# Gradient Descent loop\n",
    "for i in range(epochs):\n",
    "    Y_pred = m * X + b\n",
    "    error = Y_pred - Y\n",
    "\n",
    "    dm = (2/n) * np.dot(error, X)\n",
    "    db = (2/n) * np.sum(error)\n",
    "\n",
    "    m -= alpha * dm\n",
    "    b -= alpha * db\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        mse = np.mean(error**2)\n",
    "        print(f\"Epoch {i}: MSE = {mse:.2f}, m = {m:.2f}, b = {b:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Important Notes:\n",
    "\n",
    "* If $\\alpha$ (learning rate) is **too high** ‚Üí model may overshoot the minimum and fail to converge.\n",
    "* If it‚Äôs **too low** ‚Üí model will converge very slowly.\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ Summary:\n",
    "\n",
    "* Gradient Descent **adjusts** parameters to reduce cost.\n",
    "* It‚Äôs how a model ‚Äú**learns**‚Äù from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b354c0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
